clc
clear
close all
DATA=xlsread('H:\Soil_EC_sampling_points.csv');
B10SITPDI=DATA(:,6:8);

EC=DATA(:,1);
best_r_squared = -Inf;
best_net = [];
best_tr = [];
best_indices = [];
best_metrics = [];
best_x_combined = [];
best_t_combined = [];
best_y_combined = [];

% 启动并行池
if isempty(gcp('nocreate'))
    parpool('local');
end

% Run the loop for 100 iterations
for iter = 1:1000
    % Your existing code for creating and training the neural network
    x = B10SITPDI'; % Replace with your data
    t = EC'; % Replace with your data

    % 定义训练函数列表
    trainFcns = {'trainlm', 'trainbr', 'trainscg', 'trainrp', 'trainbfg'};
    % 设置隐藏层节点数范围
    hiddenLayerSizes = 10:10:50;%, 150:50:500];
    learningRates = [0.1, 0.01, 0.001, 0.0001]; % 学习速率范围
    epochCounts = 100:100:700; % 训练次数范围
    for trainFcn = trainFcns
        for hiddenLayerSize = hiddenLayerSizes
            for lr = learningRates
                for epochs = epochCounts
                    net = fitnet(hiddenLayerSize, trainFcn{1});
                    net.input.processFcns = {'removeconstantrows','mapminmax'};
                    net.output.processFcns = {'removeconstantrows','mapminmax'};

                    net.divideFcn = 'dividerand';  % Divide data randomly
                    net.divideMode = 'sample';
                    net.divideParam.trainRatio = 80/100;
                    net.divideParam.valRatio = 0/100;
                    net.divideParam.testRatio = 20/100;

                    %设置训练参数
                    net.trainparam.show = 1000 ; % 现实频率，这里设置为没训练20次显示一次
                    net.trainparam.epochs = epochs ;% 训练次数，这里设置为300次
                    net.trainparam.goal = 1e-3 ; % 训练目标最小误差，这里设置为0.1
                    net.trainParam.lr = lr ; % 学习速率，这里设置为0.05
                    net.trainParam.mu = 0.9 ;
                    net.trainParam.mc=0.9;%动量因子的设置，默认为0.9

                    net.performFcn = 'mse';

                    net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
                        'plotregression', 'plotfit'};

                    [net,tr] = train(net, x, t);


                    y = net(x);
                    e = gsubtract(t, y);

                    testX  = x(:, tr.testInd);
                    testT  = t(:, tr.testInd);
                    testY  = net(testX );
                    testR2 = 1 - sum((testT  - testY ).^2) / sum((testT  - mean(testT )).^2);
                    testRMSEprint = sqrt(mean((testT - testY).^2));

                    % Check if this iteration gives a better R² value
                    if testR2 > best_r_squared
                        best_r_squared = testR2;
                        best_net = net;
                        best_tr = tr;
                        best_indices = tr.testInd;

                        % Calculate other metrics for the best iteration
                        testRMSE = sqrt(mean((testT - testY).^2));
                        trainPerformance = perform(net, t .* tr.trainMask{1}, y);
                        valPerformance = perform(net, t .* tr.valMask{1}, y);
                        testPerformance = perform(net, t .* tr.testMask{1}, y);

                        best_metrics = struct('testRMSE', testRMSE, ...
                                              'trainPerformance', trainPerformance, ...
                                              'valPerformance', valPerformance, ...
                                              'testPerformance', testPerformance);

                        % Store data for the best iteration
                        best_x_combined = [ x(:, tr.testInd)];
                        best_t_combined = [ t(:, tr.testInd)];
                        best_y_combined = [ y(:, tr.testInd)];
                    else
                        % If not the best, clear the variables to free memory
                        clear net tr y e testX testT testY
                        % Force garbage collection
                        java.lang.System.gc();
                    end
                    disp([num2str(iter),'次',num2str(hiddenLayerSize),'节点隐藏层RMSE:', num2str(testRMSEprint), ', Learning Rate: ', num2str(lr), ', trainFcn: ', trainFcn{1}, ', 训练次数: ', num2str(epochs)]);
                    disp([num2str(iter),'次',num2str(hiddenLayerSize),'节点隐藏层R^2:', num2str(testR2)]);
                    disp(' ');
                end
            end
        end
    end
end

disp('Best iteration metrics:');
disp(['MSE: ', num2str(best_metrics.testPerformance)]);
disp(['R²: ', num2str(best_r_squared)]);
% disp(['训练函数: ', best_metrics.trainFcn]);
% disp(['隐藏层大小: [', num2str(best_metrics.hiddenLayerSize)]);
% disp(['学习速率: ', num2str(best_metrics.learningRate)]);
% disp(['训练次数: ', num2str(best_metrics.epochs)]);
disp(' ');

figure;
hold on;
line([0 6], [0 6], 'Color', 'r', 'LineStyle', '--', 'LineWidth', 1);
plot(best_t_combined, best_y_combined, 'bo', 'LineWidth', 1, 'LineStyle', 'none');
line([6 6], [0 6], 'Color', 'k');
line([0 6], [6 6], 'Color', 'k');
title('EC result(ds/m)');  
xlabel('Observed results');  
ylabel('Predict results');
grid off;

% Calculate metrics using the best results
rmse_test = sqrt(mean((best_t_combined - best_y_combined).^2));
rmse_test = sprintf('%.3f', rmse_test);
r_squared_combined = sprintf('%.3f', best_r_squared);

% Display RMSE and R² on the plot
rmse_text = ['RMSE=', num2str(rmse_test)];
r_squared_text = ['R²=', num2str(r_squared_combined)];
text(2.3, 5.3, {rmse_text, r_squared_text}, 'FontSize', 10, 'BackgroundColor', 'w');

% Adjust plot settings
set(gcf, 'Position', [300, 300, 420, 420]);
ax = gca;
ax.Position = [0.13, 0.13, 0.8, 0.8];
xlim([0 6]);
ylim([0 6]);
xticks(0:1:6);
yticks(0:1:6);

hold off;
